# Setting up the cluster



## Set up control plane

1. On each of the control nodes create the following directories
$ sudo mkdir /etc/haproxy /etc/keepalived

2. On each of the control nodes copy the following files

haproxy.cfg -> /etc/haproxy/
haproxy.yaml -> /etc/kubernetes/manifests/
keepalived.yaml -> /etc/kubernetes/manifests/
check_apiserver.sh -> /etc/keepalived/

3. Pick one of the nodes as the master and copy

keepalive.master.conf -> /etc/keepalived/keepalived.conf

4. On the remaining control nodes copy

keepalive.backup.conf -> /etc/keepalived/keepalived.conf

5. Enable the kublet service so it start automatically on start

$ sudo systemctl enable kubelet.service

5. On a control node initialise the cluster (note on my setup I had to turn off
   the other VM's and assign more memory to get this to complete)

$ sudo kubeadm init \
  --pod-network-cidr=192.168.0.0/16 \ 
  --control-plane-endpoint 172.21.111.111:8443 \
  --ignore-preflight-errors=mem

The network cidr tells kubeadm what address range the cni will be using
The control plane endpoint is the virtual ip haproxy is listening on, note you
need to use a different port from the default (6443) if haproxy is running on a 
control plane node
The last arg is only necessary if you're using dynamic memory for your vm and
it is not currently assigned enough memory to do the install.

6. Take a note of the join commands, there should be one for worker nodes and
   one for control nodes

7. To run kubectl as non root

$ mkdir -p $HOME/.kube
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config

## Setup the network plugin

1. Create the calico operator
$ kubectl create -f https://projectcalico.docs.tigera.io/manifests/tigera-operator.yaml
$ kubectl create -f https://projectcalico.docs.tigera.io/manifests/custom-resources.yaml

## Install calico control
Is this necessary?

1. Download it
$ curl -L https://github.com/projectcalico/calico/releases/download/v3.23.1/calicoctl-linux-amd64 -o kubectl-calico

2. Put it someowhere on your path and make it executable
$ chmod +x kubectl-calico
$ chown root:root kubectl-calico
$ sudo mv kubectl-calico /usr/bin/

## Add worker nodes
On each of the worker nodes run
```
$ sudo systemctl enable kubelet.service
$ sudo kubeadm join <control node ip> --token <your token> --discovery-token-ca-cert-hash <your hash>
```

Then on the control plane run 

$ kubectl get nodes -o wide

And you should see the new nodes
